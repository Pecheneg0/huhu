import torch
import cv2
import numpy as np
import matplotlib.pyplot as plt
from model import ArmenianLetterNet  # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å

# üîπ –ü–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
confidence_threshold = 0.55

# üîπ –ê—Ä–º—è–Ω—Å–∫–∏–π –∞–ª—Ñ–∞–≤–∏—Ç (–æ—Ç –Æ–Ω–∏–∫–æ–¥–∞ 1329 –¥–æ 1366)
armenian_alphabet = [chr(code) for code in range(0x0531, 0x0557)]

# üîπ –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
model = ArmenianLetterNet()
model.load_state_dict(torch.load("armenian_letters_model_2.pth", map_location="cpu"))
model.eval()

# üîπ –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–µ—Å—Ç–æ–≤–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
image_path = "/Users/aleksandr/Desktop/–†–∞–±–æ—Ç–∞/–°–ö–ê–¢/test_images/–°–Ω–∏–º–æ–∫ —ç–∫—Ä–∞–Ω–∞ 2025-02-28 –≤ 09.51.44.png"  # –£–∫–∞–∂–∏ –ø—É—Ç—å –∫ —Ç–µ—Å—Ç–æ–≤–æ–º—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é
image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
image = cv2.resize(image, (32, 32))
image_tensor = torch.tensor(image, dtype=torch.float32).unsqueeze(0).unsqueeze(0) / 255.0

# üîπ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
with torch.no_grad():
    output = model(image_tensor)
    probabilities = torch.nn.functional.softmax(output, dim=1)
    confidence, predicted_class = torch.max(probabilities, 1)

confidence_value = confidence.item()
predicted_index = predicted_class.item()

# üîπ –ü—Ä–æ–≤–µ—Ä—è–µ–º, –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω –ª–∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –∏–Ω–¥–µ–∫—Å
if 0 <= predicted_index < len(armenian_alphabet):
    predicted_letter = armenian_alphabet[predicted_index]
else:
    predicted_letter = "‚ùå –û—à–∏–±–∫–∞ –∏–Ω–¥–µ–∫—Å–∞"

# üîπ –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
if confidence_value >= confidence_threshold:
    text_color = "green"
    result_text = f"‚úÖ –†–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω–∞—è –±—É–∫–≤–∞: {predicted_letter} (–ò–Ω–¥–µ–∫—Å: {predicted_index}) | –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence_value:.2f}"
else:
    text_color = "red"
    result_text = f"‚ö†Ô∏è –ë—É–∫–≤–∞ –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–∞ —Å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é! ({confidence_value:.2f})"

# üîπ –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
image_rgb = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Ü–≤–µ—Ç–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
plt.figure(figsize=(5, 5))
plt.imshow(image_rgb, cmap="gray")
plt.axis("off")
plt.title(result_text, color=text_color, fontsize=12, weight="bold")
plt.show()




def predict_multiple_frames(model, images):
    """ –£—Å—Ä–µ–¥–Ω—è–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. """
    predictions = []
    for image in images:
        image_tensor = torch.tensor(image, dtype=torch.float32).unsqueeze(0).unsqueeze(0) / 255.0
        with torch.no_grad():
            output = model(image_tensor)
            _, predicted_class = torch.max(output, 1)
            predictions.append(predicted_class.item())

    final_prediction = max(set(predictions), key=predictions.count)  # ‚úÖ –ë–µ—Ä—ë–º –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç—ã–π –∫–ª–∞—Å—Å
    return final_prediction





# üîπ –í—ã–≤–æ–¥ –≤ –∫–æ–Ω—Å–æ–ª—å
print("üìå **–†–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è:**")
print(f"üî¢ –ò–Ω–¥–µ–∫—Å –±—É–∫–≤—ã: {predicted_index} | üî§ –†–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω–∞—è –±—É–∫–≤–∞: {predicted_letter}")
print(f"üìä –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏: {confidence_value:.2f}")

# üîπ –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ, –µ—Å–ª–∏ –∏–Ω–¥–µ–∫—Å –≤—ã—Ö–æ–¥–∏—Ç –∑–∞ –≥—Ä–∞–Ω–∏—Ü—ã –∞–ª—Ñ–∞–≤–∏—Ç–∞
if predicted_letter == "‚ùå –û—à–∏–±–∫–∞ –∏–Ω–¥–µ–∫—Å–∞":
    print("‚ö†Ô∏è –û—à–∏–±–∫–∞! –ò–Ω–¥–µ–∫—Å –±—É–∫–≤—ã –≤—ã—Ö–æ–¥–∏—Ç –∑–∞ –ø—Ä–µ–¥–µ–ª—ã –¥–æ–ø—É—Å—Ç–∏–º–æ–≥–æ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ (0-35).")
